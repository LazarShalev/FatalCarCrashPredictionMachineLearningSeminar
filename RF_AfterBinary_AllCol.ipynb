{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions\n",
    "from pyspark.sql.functions import col,when\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxu/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "accident = pd.read_csv('/home/linuxu/Desktop/1.csv')\n",
    "modifiedAccident = accident.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter\n",
    "modifiedAccident1 = modifiedAccident[modifiedAccident['Road_Type'] >= 0]\n",
    "modifiedAccident2 = modifiedAccident1[modifiedAccident1['Speed_limit'] >= 0]\n",
    "modifiedAccident3 = modifiedAccident2[modifiedAccident2['Junction_Detail'] >= 0]\n",
    "modifiedAccident4 = modifiedAccident3[modifiedAccident3['2nd_Road_Class'] >= 0]\n",
    "modifiedAccident5 = modifiedAccident4[modifiedAccident4['Junction_Control'] >= 0]\n",
    "modifiedAccident6 = modifiedAccident5[modifiedAccident5['Light_Conditions'] >= 0]\n",
    "modifiedAccident7 = modifiedAccident6[modifiedAccident6['Weather_Conditions'] >= 0]\n",
    "modifiedAccident8 = modifiedAccident7[modifiedAccident7['Road_Surface_Conditions'] >= 0]\n",
    "modifiedAccident9 = modifiedAccident8[modifiedAccident8['Special_Conditions_at_Site'] >= 0]\n",
    "modifiedAccident10 = modifiedAccident9[modifiedAccident9['1st_Road_Class'] >= 0]\n",
    "#filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose data ammount\n",
    "subData = modifiedAccident10[0:1200000]\n",
    "\n",
    "subData.to_csv('/home/linuxu/Desktop/subData.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "allColumns = ['Road_Type', 'Speed_limit', 'Junction_Detail', '2nd_Road_Class', \n",
    "              '1st_Road_Class', 'Junction_Control','Light_Conditions','Weather_Conditions'\n",
    "              ,'Road_Surface_Conditions','Special_Conditions_at_Site']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Light_Conditions', 'Weather_Conditions', '1st_Road_Class', 'Junction_Detail', 'Special_Conditions_at_Site', 'Speed_limit', 'Road_Type', 'Road_Surface_Conditions', '2nd_Road_Class', 'Junction_Control']\n",
      "Data splitted\n",
      "1 pass completed\n",
      "['Speed_limit', 'Junction_Detail', 'Road_Surface_Conditions', 'Road_Type', 'Light_Conditions', 'Junction_Control', '2nd_Road_Class', 'Weather_Conditions', '1st_Road_Class', 'Special_Conditions_at_Site']\n",
      "Data splitted\n",
      "2 pass completed\n",
      "['Road_Surface_Conditions', 'Junction_Control', 'Road_Type', 'Speed_limit', 'Special_Conditions_at_Site', 'Junction_Detail', '1st_Road_Class', 'Weather_Conditions', '2nd_Road_Class', 'Light_Conditions']\n",
      "Data splitted\n",
      "3 pass completed\n",
      "['Junction_Detail', 'Road_Type', 'Weather_Conditions', '2nd_Road_Class', 'Speed_limit', 'Special_Conditions_at_Site', 'Junction_Control', '1st_Road_Class', 'Light_Conditions', 'Road_Surface_Conditions']\n",
      "Data splitted\n",
      "4 pass completed\n",
      "['Speed_limit', 'Weather_Conditions', 'Junction_Control', '2nd_Road_Class', 'Road_Type', '1st_Road_Class', 'Light_Conditions', 'Special_Conditions_at_Site', 'Road_Surface_Conditions', 'Junction_Detail']\n",
      "Data splitted\n",
      "5 pass completed\n",
      "['Light_Conditions', 'Junction_Detail', 'Road_Surface_Conditions', 'Junction_Control', 'Special_Conditions_at_Site', 'Road_Type', 'Speed_limit', '1st_Road_Class', 'Weather_Conditions', '2nd_Road_Class']\n",
      "Data splitted\n",
      "6 pass completed\n",
      "['Light_Conditions', 'Junction_Detail', 'Road_Type', 'Road_Surface_Conditions', '2nd_Road_Class', 'Special_Conditions_at_Site', '1st_Road_Class', 'Weather_Conditions', 'Speed_limit', 'Junction_Control']\n",
      "Data splitted\n",
      "7 pass completed\n",
      "['Special_Conditions_at_Site', 'Light_Conditions', 'Junction_Control', 'Road_Type', 'Weather_Conditions', '2nd_Road_Class', '1st_Road_Class', 'Junction_Detail', 'Road_Surface_Conditions', 'Speed_limit']\n",
      "Data splitted\n",
      "8 pass completed\n",
      "['Road_Type', 'Junction_Detail', 'Junction_Control', '1st_Road_Class', 'Light_Conditions', 'Weather_Conditions', '2nd_Road_Class', 'Speed_limit', 'Road_Surface_Conditions', 'Special_Conditions_at_Site']\n",
      "Data splitted\n",
      "9 pass completed\n",
      "['Road_Type', '1st_Road_Class', '2nd_Road_Class', 'Road_Surface_Conditions', 'Light_Conditions', 'Junction_Detail', 'Speed_limit', 'Special_Conditions_at_Site', 'Junction_Control', 'Weather_Conditions']\n",
      "Data splitted\n",
      "10 pass completed\n"
     ]
    }
   ],
   "source": [
    "avg = 0\n",
    "sum = 0\n",
    "\n",
    "i = 1\n",
    "while i <= 10:\n",
    "    \n",
    "    #open a spark session\n",
    "  spark = SparkSession.builder.appName('ml-bank').getOrCreate()\n",
    "\n",
    "    #read the data\n",
    "  df = spark.read.csv('/home/linuxu/Desktop/subData.csv', header = True, inferSchema = True)\n",
    "    \n",
    "    #random select from columns\n",
    "  comb = random.sample(allColumns, k = 10)\n",
    "\n",
    "    #print the combinations that was chosen\n",
    "  print(comb)\n",
    "\n",
    "    #open and write file\n",
    "  fc = open(\"/home/linuxu/Desktop/CombinationsTried.txt\",\"a+\")\n",
    "  sentence = str(i)+ \".  \"+ ', '.join(map(str, comb))  + \"\\n\"\n",
    "  fc.write(sentence)\n",
    "  fc.close()\n",
    "    \n",
    "      #binary\n",
    "  df = df.withColumn(\"Accident_Severity\", when(col(\"Accident_Severity\")  == 3,0).otherwise(1))\n",
    "\n",
    "    #select the first col to be accident sevirity and the rest are random\n",
    "  df = df.select('Accident_Severity', comb[0], comb[1], comb[2], comb[3],comb[4],comb[5],comb[6],comb[7],comb[8],comb[9])\n",
    "  cols = df.columns\n",
    "\n",
    "    #diffrintiate between strings and int in the data\n",
    "  categoricalColumns = []\n",
    "  numericCols = []\n",
    "  for cmb in range(10):\n",
    "    if df.dtypes[cmb+1][1] == 'string':\n",
    "      categoricalColumns.append(cols[cmb+1])\n",
    "    elif df.dtypes[cmb+1][1] == 'int':\n",
    "      numericCols.append(cols[cmb+1])\n",
    "\n",
    "\n",
    "  stages = []\n",
    "    \n",
    "  for categoricalCol in categoricalColumns:\n",
    "      stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "      encoder = OneHotEncoderEstimator(inputCols= [stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "      stages += [stringIndexer, encoder]\n",
    "\n",
    "  label_stringIdx = StringIndexer(inputCol = 'Accident_Severity', outputCol = 'label')\n",
    "  stages += [label_stringIdx]\n",
    "\n",
    "  assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "  assembler = VectorAssembler(inputCols = assemblerInputs, outputCol=\"features\")\n",
    "  stages += [assembler]\n",
    "\n",
    "  pipeline = Pipeline(stages = stages)\n",
    "  pipelineModel = pipeline.fit(df)\n",
    "    \n",
    "  df = pipelineModel.transform(df)\n",
    "  selectedCols = ['label', 'features'] + cols\n",
    "    \n",
    "  df = df.select(selectedCols)\n",
    "\n",
    "    #split the DATA\n",
    "  train, test = df.randomSplit([0.8, 0.2])\n",
    "  print(\"Data splitted\")\n",
    "\n",
    "    \n",
    "    ###################\n",
    "####### algorithems ########\n",
    "    ###################\n",
    "    \n",
    "  #Random Forest Classifier\n",
    "  rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "  rfModel = rf.fit(train)\n",
    "  predictionsRF = rfModel.transform(test)\n",
    "\n",
    "  resultsRF = predictionsRF.select(['prediction', 'label'])\n",
    "  predictionAndLabelsRF = resultsRF.rdd\n",
    "  metricsRF = MulticlassMetrics(predictionAndLabelsRF)\n",
    "\n",
    "  fr = open(\"/home/linuxu/Desktop/results.txt\",\"a+\")\n",
    "  fr.write(\"Column Combination: \"+ ', '.join(map(str, comb)) + \"\\n\")\n",
    "  fr.write(\"Results for Random Forest Algorithem: \\n\")\n",
    "  fr.write(\"Accuracy: \" + str((metricsRF.accuracy)*100) + \"%\\n\")\n",
    "  fr.write(\"Precision: \" + str(metricsRF.weightedPrecision) + \"\\n\")\n",
    "  fr.write(\"Recall: \" + str(metricsRF.weightedRecall) + \"\\n\")\n",
    "  fr.write(\"TruePositiveRate: \" + str(metricsRF.weightedTruePositiveRate) + \"\\n\")\n",
    "  fr.write(\"FalsePositiveRate: \" + str(metricsRF.weightedFalsePositiveRate) + \"\\n\\n\\n\\n\")\n",
    "  fr.close()\n",
    "    \n",
    "  filename = \"/home/linuxu/Desktop/comb\" + str(i) + \"_RF.csv\"\n",
    "  predictionsRF.toPandas().to_csv(filename)\n",
    "  print(\"RF done\")\n",
    "\n",
    "  sum += (metricsRF.accuracy)*100\n",
    "    \n",
    "  spark.catalog.clearCache()\n",
    "  del comb\n",
    "  del categoricalColumns\n",
    "  del numericCols\n",
    "  del stages\n",
    "  print(i, \"pass completed\")\n",
    "  i = i + 1\n",
    "    \n",
    "#write max\n",
    "fr = open(\"/home/linuxu/Desktop/results.txt\",\"a+\")\n",
    "fr.write(\"Avarage is: \" + str(sum/10))\n",
    "fr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
