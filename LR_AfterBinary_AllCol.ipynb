{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions\n",
    "from pyspark.sql.functions import col,when\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxu/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "accident = pd.read_csv('/home/linuxu/Desktop/1.csv')\n",
    "modifiedAccident = accident.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter\n",
    "modifiedAccident1 = modifiedAccident[modifiedAccident['Road_Type'] >= 0]\n",
    "modifiedAccident2 = modifiedAccident1[modifiedAccident1['Speed_limit'] >= 0]\n",
    "modifiedAccident3 = modifiedAccident2[modifiedAccident2['Junction_Detail'] >= 0]\n",
    "modifiedAccident4 = modifiedAccident3[modifiedAccident3['2nd_Road_Class'] >= 0]\n",
    "modifiedAccident5 = modifiedAccident4[modifiedAccident4['Junction_Control'] >= 0]\n",
    "modifiedAccident6 = modifiedAccident5[modifiedAccident5['Light_Conditions'] >= 0]\n",
    "modifiedAccident7 = modifiedAccident6[modifiedAccident6['Weather_Conditions'] >= 0]\n",
    "modifiedAccident8 = modifiedAccident7[modifiedAccident7['Road_Surface_Conditions'] >= 0]\n",
    "modifiedAccident9 = modifiedAccident8[modifiedAccident8['Special_Conditions_at_Site'] >= 0]\n",
    "modifiedAccident10 = modifiedAccident9[modifiedAccident9['1st_Road_Class'] >= 0]\n",
    "#filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose data ammount\n",
    "subData = modifiedAccident10[0:1200000]\n",
    "\n",
    "subData.to_csv('/home/linuxu/Desktop/subData.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "allColumns = ['Road_Type', 'Speed_limit', 'Junction_Detail', '2nd_Road_Class', \n",
    "              '1st_Road_Class', 'Junction_Control','Light_Conditions','Weather_Conditions'\n",
    "              ,'Road_Surface_Conditions','Special_Conditions_at_Site']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Light_Conditions', '2nd_Road_Class', 'Road_Surface_Conditions', 'Road_Type', 'Junction_Detail', '1st_Road_Class', 'Speed_limit', 'Weather_Conditions', 'Special_Conditions_at_Site', 'Junction_Control']\n",
      "Data splitted\n",
      "LR pridictions saving...\n",
      "LR done\n",
      "1 pass completed\n",
      "['Road_Surface_Conditions', 'Special_Conditions_at_Site', 'Light_Conditions', 'Junction_Control', 'Junction_Detail', 'Speed_limit', '1st_Road_Class', 'Weather_Conditions', 'Road_Type', '2nd_Road_Class']\n",
      "Data splitted\n",
      "LR pridictions saving...\n",
      "LR done\n",
      "2 pass completed\n",
      "['Road_Type', '2nd_Road_Class', 'Junction_Control', '1st_Road_Class', 'Junction_Detail', 'Light_Conditions', 'Speed_limit', 'Road_Surface_Conditions', 'Weather_Conditions', 'Special_Conditions_at_Site']\n",
      "Data splitted\n",
      "LR pridictions saving...\n",
      "LR done\n",
      "3 pass completed\n",
      "['Weather_Conditions', 'Special_Conditions_at_Site', 'Junction_Control', 'Road_Surface_Conditions', 'Road_Type', '2nd_Road_Class', 'Junction_Detail', 'Light_Conditions', '1st_Road_Class', 'Speed_limit']\n",
      "Data splitted\n",
      "LR pridictions saving...\n",
      "LR done\n",
      "4 pass completed\n",
      "['Special_Conditions_at_Site', 'Light_Conditions', 'Weather_Conditions', '1st_Road_Class', 'Road_Surface_Conditions', '2nd_Road_Class', 'Road_Type', 'Junction_Control', 'Speed_limit', 'Junction_Detail']\n",
      "Data splitted\n",
      "LR pridictions saving...\n",
      "LR done\n",
      "5 pass completed\n",
      "['Speed_limit', 'Special_Conditions_at_Site', 'Weather_Conditions', 'Junction_Detail', 'Road_Surface_Conditions', 'Light_Conditions', 'Junction_Control', '2nd_Road_Class', '1st_Road_Class', 'Road_Type']\n",
      "Data splitted\n",
      "LR pridictions saving...\n",
      "LR done\n",
      "6 pass completed\n",
      "['Speed_limit', '2nd_Road_Class', 'Junction_Detail', 'Road_Surface_Conditions', 'Light_Conditions', 'Special_Conditions_at_Site', 'Weather_Conditions', 'Junction_Control', 'Road_Type', '1st_Road_Class']\n",
      "Data splitted\n",
      "LR pridictions saving...\n",
      "LR done\n",
      "7 pass completed\n",
      "['Junction_Control', 'Special_Conditions_at_Site', '1st_Road_Class', 'Junction_Detail', 'Light_Conditions', '2nd_Road_Class', 'Speed_limit', 'Road_Surface_Conditions', 'Road_Type', 'Weather_Conditions']\n",
      "Data splitted\n",
      "LR pridictions saving...\n",
      "LR done\n",
      "8 pass completed\n",
      "['Road_Surface_Conditions', '1st_Road_Class', 'Weather_Conditions', 'Junction_Control', 'Road_Type', '2nd_Road_Class', 'Junction_Detail', 'Special_Conditions_at_Site', 'Speed_limit', 'Light_Conditions']\n",
      "Data splitted\n",
      "LR pridictions saving...\n",
      "LR done\n",
      "9 pass completed\n",
      "['Road_Surface_Conditions', 'Junction_Control', 'Weather_Conditions', 'Road_Type', 'Junction_Detail', 'Special_Conditions_at_Site', 'Light_Conditions', '1st_Road_Class', 'Speed_limit', '2nd_Road_Class']\n",
      "Data splitted\n",
      "LR pridictions saving...\n",
      "LR done\n",
      "10 pass completed\n"
     ]
    }
   ],
   "source": [
    "avg = 0\n",
    "sum = 0\n",
    "\n",
    "i = 1\n",
    "while i <= 10:\n",
    "    \n",
    "    #open a spark session\n",
    "  spark = SparkSession.builder.appName('ml-bank').getOrCreate()\n",
    "\n",
    "    #read the data\n",
    "  df = spark.read.csv('/home/linuxu/Desktop/subData.csv', header = True, inferSchema = True)\n",
    "\n",
    "    #random select from columns\n",
    "  comb = random.sample(allColumns, k = 10)\n",
    "\n",
    "    #print the combinations that was chosen\n",
    "  print(comb)\n",
    "\n",
    "    #open and write file\n",
    "  fc = open(\"/home/linuxu/Desktop/CombinationsTried.txt\",\"a+\")\n",
    "  sentence = str(i)+ \".  \"+ ', '.join(map(str, comb))  + \"\\n\"\n",
    "  fc.write(sentence)\n",
    "  fc.close()\n",
    "\n",
    "    \n",
    "  #binary\n",
    "  df = df.withColumn(\"Accident_Severity\", when(col(\"Accident_Severity\")  == 3,0).otherwise(1))\n",
    "\n",
    "  df = df.select('Accident_Severity', comb[0], comb[1], comb[2], comb[3],comb[4],comb[5],comb[6],comb[7],comb[8],comb[9])  \n",
    "    \n",
    "  cols = df.columns\n",
    "\n",
    "    #diffrintiate between strings and int in the data\n",
    "  categoricalColumns = []\n",
    "  numericCols = []\n",
    "  for cmb in range(10):\n",
    "    if df.dtypes[cmb+1][1] == 'string':\n",
    "      categoricalColumns.append(cols[cmb+1])\n",
    "    elif df.dtypes[cmb+1][1] == 'int':\n",
    "      numericCols.append(cols[cmb+1])\n",
    "\n",
    "  stages = []\n",
    "    \n",
    "    #Preparing Data for Machine Learning\n",
    "  for categoricalCol in categoricalColumns:\n",
    "      stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "      encoder = OneHotEncoderEstimator(inputCols= [stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "      stages += [stringIndexer, encoder]\n",
    "\n",
    "  label_stringIdx = StringIndexer(inputCol = 'Accident_Severity', outputCol = 'label')\n",
    "  stages += [label_stringIdx]\n",
    "\n",
    "  assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "  assembler = VectorAssembler(inputCols = assemblerInputs, outputCol=\"features\")\n",
    "  stages += [assembler]\n",
    "\n",
    "    #Pipeline\n",
    "  pipeline = Pipeline(stages = stages)\n",
    "  pipelineModel = pipeline.fit(df)\n",
    "    \n",
    "  df = pipelineModel.transform(df)\n",
    "  selectedCols = ['label', 'features'] + cols\n",
    "    \n",
    "  df = df.select(selectedCols)\n",
    "\n",
    "    #split the DATA\n",
    "  train, test = df.randomSplit([0.8, 0.2])\n",
    "  print(\"Data splitted\")\n",
    "\n",
    "    \n",
    "    ###################\n",
    "####### algorithems ########\n",
    "    ###################\n",
    "    \n",
    "  #Logistic Regression Model\n",
    "  lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=100)\n",
    "  lrModel = lr.fit(train)\n",
    "  predictions = lrModel.transform(test)\n",
    "  print(\"LR pridictions saving...\")\n",
    "\n",
    "  filename = \"comb\" + str(i) + \"_LR.csv\"\n",
    "  predictions.toPandas().to_csv(\"/home/linuxu/Desktop/\"+filename)\n",
    "  print(\"LR predicitions saved\")\n",
    "\n",
    "  #Evaluation Measures\n",
    "  trainingSummary = lrModel.summary\n",
    "\n",
    "  fr = open(\"/home/linuxu/Desktop/results.txt\",\"a+\")\n",
    "  fr.write(\"Column Combination: \"+ ', '.join(map(str, comb)) + \"\\n\")\n",
    "  fr.write(\"Results for Logistic Regression: \\n\")\n",
    "  fr.write(\"Accuracy: \" + str((trainingSummary.accuracy)*100) + \"\\n\")\n",
    "  fr.write(\"Precision: \" + str(trainingSummary.weightedPrecision) + \"\\n\")\n",
    "  fr.write(\"Recall: \" + str(trainingSummary.weightedRecall) + \"\\n\")\n",
    "  fr.write(\"TruePositiveRate: \" + str(trainingSummary.weightedTruePositiveRate) + \"\\n\")\n",
    "  fr.write(\"FalsePositiveRate: \" + str(trainingSummary.weightedFalsePositiveRate) + \"\\n\\n\\n\\n\")\n",
    "  fr.close()\n",
    "\n",
    "  print(\"LR done\")\n",
    "\n",
    "  sum += (trainingSummary.accuracy)*100\n",
    "\n",
    "  spark.catalog.clearCache()\n",
    "  del comb\n",
    "  del categoricalColumns\n",
    "  del numericCols\n",
    "  del stages\n",
    "  print(i, \"pass completed\")\n",
    "  i = i + 1\n",
    "\n",
    "#write max\n",
    "fr = open(\"/home/linuxu/Desktop/results.txt\",\"a+\")\n",
    "fr.write(\"Avarage is: \" + str(sum/10))\n",
    "fr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
