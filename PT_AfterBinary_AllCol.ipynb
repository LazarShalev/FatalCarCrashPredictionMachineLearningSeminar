{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import pyspark.sql.functions\n",
    "from pyspark.sql.functions import col,when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxu/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "accident = pd.read_csv('/home/linuxu/Desktop/1.csv')\n",
    "modifiedAccident = accident.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter\n",
    "modifiedAccident1 = modifiedAccident[modifiedAccident['Road_Type'] >= 0]\n",
    "modifiedAccident2 = modifiedAccident1[modifiedAccident1['Speed_limit'] >= 0]\n",
    "modifiedAccident3 = modifiedAccident2[modifiedAccident2['Junction_Detail'] >= 0]\n",
    "modifiedAccident4 = modifiedAccident3[modifiedAccident3['2nd_Road_Class'] >= 0]\n",
    "modifiedAccident5 = modifiedAccident4[modifiedAccident4['Junction_Control'] >= 0]\n",
    "modifiedAccident6 = modifiedAccident5[modifiedAccident5['Light_Conditions'] >= 0]\n",
    "modifiedAccident7 = modifiedAccident6[modifiedAccident6['Weather_Conditions'] >= 0]\n",
    "modifiedAccident8 = modifiedAccident7[modifiedAccident7['Road_Surface_Conditions'] >= 0]\n",
    "modifiedAccident9 = modifiedAccident8[modifiedAccident8['Special_Conditions_at_Site'] >= 0]\n",
    "modifiedAccident10 = modifiedAccident9[modifiedAccident9['1st_Road_Class'] >= 0]\n",
    "#filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subData = modifiedAccident[0:1200000]\n",
    "subData.to_csv('/home/linuxu/Desktop/subData.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "allColumns = ['1st_Road_Class', 'Road_Type', 'Speed_limit', 'Junction_Detail', 'Junction_Control', '2nd_Road_Class', 'Light_Conditions', 'Weather_Conditions', \n",
    "              'Road_Surface_Conditions', 'Special_Conditions_at_Site']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Algo done\n",
      "1 pass completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "i = 1\n",
    "while i <= 1:\n",
    "  spark = SparkSession.builder.appName('ml-bank').getOrCreate()\n",
    "  df = spark.read.csv('/home/linuxu/Desktop/subData.csv', header = True, inferSchema = True)\n",
    "  comb = random.sample(allColumns, k = 10)\n",
    "\n",
    "  fc = open(\"/home/linuxu/Desktop/CombinationsTried.txt\",\"a+\")\n",
    "  sentence = str(i)+ \".  \"+ ', '.join(map(str, comb))  + \"\\n\"\n",
    "  fc.write(sentence)\n",
    "  fc.close()\n",
    "\n",
    "    #binary\n",
    "  df = df.withColumn(\"Accident_Severity\", when(col(\"Accident_Severity\")  == 3,0).otherwise(1))\n",
    "\n",
    "  df = df.select('Accident_Severity', comb[0], comb[1], comb[2], comb[3], comb[4], comb[5], comb[6], comb[7], comb[8], comb[9])\n",
    "  cols = df.columns\n",
    "\n",
    "\n",
    "  categoricalColumns = []\n",
    "  numericCols = []\n",
    "  for cmb in range(10):\n",
    "    if df.dtypes[cmb+1][1] == 'string':\n",
    "      categoricalColumns.append(cols[cmb+1])\n",
    "    elif df.dtypes[cmb+1][1] == 'int':\n",
    "      numericCols.append(cols[cmb+1])\n",
    "\n",
    "  stages = []\n",
    "\n",
    "  for categoricalCol in categoricalColumns:\n",
    "      stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "      encoder = OneHotEncoderEstimator(inputCols= [stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "      stages += [stringIndexer, encoder]\n",
    "\n",
    "  label_stringIdx = StringIndexer(inputCol = 'Accident_Severity', outputCol = 'label')\n",
    "  stages += [label_stringIdx]\n",
    "\n",
    "  assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "  assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "  stages += [assembler]\n",
    "\n",
    "\n",
    "  pipeline = Pipeline(stages = stages)\n",
    "  pipelineModel = pipeline.fit(df)\n",
    "  df = pipelineModel.transform(df)\n",
    "  selectedCols = ['label', 'features'] + cols\n",
    "  df = df.select(selectedCols)\n",
    "\n",
    "  train, test = df.randomSplit([0.8, 0.2])\n",
    "\n",
    "\n",
    "  #Perceptron Algo\n",
    "  layers = [10, 5, 6, 2]\n",
    "  mlpc = MultilayerPerceptronClassifier(layers = layers, blockSize = 32, seed = 4, featuresCol = 'features', labelCol = 'label')\n",
    "  model = mlpc.fit(train)\n",
    "  # compute accuracy on the test set\n",
    "  result = model.transform(test)\n",
    "  filename = \"/home/linuxu/Desktop/comb\" + str(i) + \"_Perceptron.csv\"\n",
    "  result.toPandas().to_csv(filename)\n",
    "\n",
    "  resultsPerceptron = result.select(['prediction', 'label'])\n",
    "  predictionAndLabels = resultsPerceptron.rdd\n",
    "  metricsPerceptron = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "  fr = open(\"/home/linuxu/Desktop/results.txt\",\"a+\")\n",
    "  fr.write(\"Column Combination: \"+ ', '.join(map(str, comb)) + \"\\n\")\n",
    "  fr.write(\"Results for Perceptron Algorithm: \\n\")\n",
    "  fr.write(\"Accuracy: \" + str((metricsPerceptron.accuracy)*100) + \"\\n\")\n",
    "  fr.write(\"Precision: \" + str(metricsPerceptron.weightedPrecision) + \"\\n\")\n",
    "  fr.write(\"Recall: \" + str(metricsPerceptron.weightedRecall) + \"\\n\")\n",
    "  fr.write(\"TruePositiveRate: \" + str(metricsPerceptron.weightedTruePositiveRate) + \"\\n\")\n",
    "  fr.write(\"FalsePositiveRate: \" + str(np.nan_to_num(metricsPerceptron.weightedFalsePositiveRate)) + \"\\n\\n\\n\\n\")\n",
    "  fr.close()\n",
    "\n",
    "  print(\"Perceptron Algo done\") \n",
    "    \n",
    "  spark.catalog.clearCache()\n",
    "  del comb\n",
    "  del categoricalColumns\n",
    "  del numericCols\n",
    "  del stages\n",
    "  print(i, \"pass completed\")\n",
    "  i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
